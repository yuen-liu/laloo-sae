{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sparse Autoencoder Interpretability Analysis\n",
        "\n",
        "This notebook provides an interactive environment for analyzing the results of the Sparse Autoencoder trained on protein-ligand docking poses. We'll explore:\n",
        "\n",
        "1. **Feature Analysis**: Understanding which latent features correlate with pose quality\n",
        "2. **Sparsity Patterns**: Analyzing the sparsity structure learned by the SAE\n",
        "3. **Visualization**: Exploring the latent space and feature relationships\n",
        "4. **Comparison**: Comparing SAE performance with traditional methods like PCA\n",
        "5. **Biological Interpretation**: Connecting learned features to molecular properties\n",
        "\n",
        "## Scientific Motivation\n",
        "\n",
        "The goal is to identify which features in the 30D latent space are most predictive of successful protein-ligand docking poses (<2Å RMSD) versus failures (≥3Å RMSD). This will help:\n",
        "- Filter poor poses early in VAE-diffusion docking pipelines\n",
        "- Understand what makes a docking pose successful\n",
        "- Guide future model development and feature engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import json\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src directory to path\n",
        "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
        "\n",
        "from data_loader import load_docking_data, create_sample_data\n",
        "from model import SparseAutoencoder, create_model\n",
        "from analysis import SAEAnalyzer\n",
        "from utils import set_seed, get_device, load_config\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Model\n",
        "\n",
        "First, let's load the configuration, data, and trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "config_path = \"../configs/config.yaml\"\n",
        "config = load_config(config_path)\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"Model hidden dim: {config['model']['hidden_dim']}\")\n",
        "print(f\"Sparsity lambda: {config['model']['sparsity_lambda']}\")\n",
        "print(f\"Data path: {config['data']['data_path']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "data_path = config['data']['data_path']\n",
        "if not os.path.exists(data_path):\n",
        "    print(\"Creating sample data...\")\n",
        "    create_sample_data(data_path)\n",
        "\n",
        "train_loader, val_loader, test_loader, scaler = load_docking_data(data_path, config['data'])\n",
        "print(f\"Data loaded successfully!\")\n",
        "print(f\"Train samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "print(f\"Test samples: {len(test_loader.dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained model (you'll need to train first or provide path to trained model)\n",
        "model_path = \"../models/best_model.pt\"  # Update this path as needed\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    device = get_device()\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model = create_model(config['model'])\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    print(f\"Model loaded from: {model_path}\")\n",
        "    print(f\"Training completed at epoch: {checkpoint.get('epoch', 'Unknown')}\")\n",
        "    print(f\"Best validation loss: {checkpoint.get('best_val_loss', 'Unknown')}\")\n",
        "else:\n",
        "    print(f\"Model not found at {model_path}. Please train a model first.\")\n",
        "    print(\"You can train a model by running: python main.py --mode train\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Extraction and Basic Analysis\n",
        "\n",
        "Let's extract features from the trained model and perform basic analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create analyzer\n",
        "analyzer = SAEAnalyzer(model, device)\n",
        "\n",
        "# Extract features from test set\n",
        "print(\"Extracting features from test set...\")\n",
        "latents, hidden_features, ga_rankings, rmsd_values, quality_labels = analyzer.extract_features(test_loader)\n",
        "\n",
        "print(f\"Extracted features shape: {hidden_features.shape}\")\n",
        "print(f\"Quality distribution: {np.bincount(quality_labels.astype(int))}\")\n",
        "print(f\"RMSD range: {rmsd_values.min():.2f} - {rmsd_values.max():.2f} Å\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze sparsity patterns\n",
        "sparsity_analysis = analyzer.analyze_sparsity(hidden_features, threshold=0.1)\n",
        "\n",
        "print(\"Sparsity Analysis:\")\n",
        "print(f\"Overall sparsity: {sparsity_analysis['overall_sparsity']:.3f}\")\n",
        "print(f\"Most sparse features: {sparsity_analysis['most_sparse_features']}\")\n",
        "print(f\"Least sparse features: {sparsity_analysis['least_sparse_features']}\")\n",
        "\n",
        "# Plot sparsity distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Sparsity per sample\n",
        "axes[0].hist(sparsity_analysis['sparsity_per_sample'], bins=50, alpha=0.7, edgecolor='black')\n",
        "axes[0].set_xlabel('Sparsity (fraction of inactive features)')\n",
        "axes[0].set_ylabel('Number of samples')\n",
        "axes[0].set_title('Sparsity Distribution Across Samples')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Sparsity per feature\n",
        "axes[1].hist(sparsity_analysis['sparsity_per_feature'], bins=50, alpha=0.7, edgecolor='black')\n",
        "axes[1].set_xlabel('Sparsity (fraction of inactive samples)')\n",
        "axes[1].set_ylabel('Number of features')\n",
        "axes[1].set_title('Sparsity Distribution Across Features')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
