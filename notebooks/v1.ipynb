{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopK Sparse Autoencoder for Protein-Ligand Docking Analysis\n",
    "\n",
    "**Goal**: Train a TopK Sparse Autoencoder on 30D VAE latent vectors to identify interpretable features distinguishing native-like poses (RMSD <2Å) from poor poses (RMSD >2Å).\n",
    "\n",
    "**Data**: ~6,000-7,000 poses per protein system, filtered to generations 0-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from scipy.stats import spearmanr\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from schrodinger.structure import StructureReader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Matplotlib inline for Jupyter\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading Section\n",
    "# Load pickle files containing latent vectors and pose quality metrics\n",
    "\n",
    "def load_pickle_data(data_dir, max_gen=7):\n",
    "    \"\"\"\n",
    "    Load latent vectors and metadata from pickle files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing .pkl files\n",
    "        max_gen: Maximum generation number to include (default: 7)\n",
    "    \n",
    "    Returns:\n",
    "        latents: numpy array of shape [N, 30] - latent vectors\n",
    "        rmsd: numpy array of shape [N] - ligand RMSD values\n",
    "        energy: numpy array of shape [N] - energy scores\n",
    "        gen: numpy array of shape [N] - generation numbers\n",
    "    \"\"\"\n",
    "    # Search for pickle files recursively in subdirectories\n",
    "    pkl_files = sorted(glob.glob(os.path.join(data_dir, '**/*.pkl'), recursive=True))\n",
    "    if not pkl_files:\n",
    "        # Fallback to non-recursive search\n",
    "        pkl_files = sorted(glob.glob(os.path.join(data_dir, '*.pkl')))\n",
    "    print(f\"Found {len(pkl_files)} pickle files\")\n",
    "    \n",
    "    latents_list = []\n",
    "    rmsd_list = []\n",
    "    energy_list = []\n",
    "    gen_list = []\n",
    "    \n",
    "    for file in pkl_files:\n",
    "        try:\n",
    "            with open(file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            # Handle different possible key names for latent vector\n",
    "            if 'z' in data:\n",
    "                z = data['z']\n",
    "            elif 'latent' in data:\n",
    "                z = data['latent']\n",
    "            else:\n",
    "                print(f\"Warning: No 'z' or 'latent' key in {file}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Handle different possible structures for scores\n",
    "            if isinstance(data.get('ligand_rmsd'), (int, float, np.number)):\n",
    "                rmsd = data['ligand_rmsd']\n",
    "            elif 'scores' in data and 'lig_rmsd' in data['scores']:\n",
    "                rmsd = data['scores']['lig_rmsd']\n",
    "            elif 'scores' in data and 'ligand_rmsd' in data['scores']:\n",
    "                rmsd = data['scores']['ligand_rmsd']\n",
    "            else:\n",
    "                print(f\"Warning: No RMSD found in {file}, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Handle energy\n",
    "            if isinstance(data.get('energy'), (int, float, np.number)):\n",
    "                energy = data['energy']\n",
    "            elif 'scores' in data and 'energy' in data['scores']:\n",
    "                energy = data['scores']['energy']\n",
    "            else:\n",
    "                energy = np.nan\n",
    "            \n",
    "            # Handle generation\n",
    "            if isinstance(data.get('curr_gen'), (int, np.integer)):\n",
    "                gen = data['curr_gen']\n",
    "            else:\n",
    "                gen = 0  # Default to generation 0 if not specified\n",
    "            \n",
    "            # Filter by generation\n",
    "            if gen > max_gen:\n",
    "                continue\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if isinstance(z, torch.Tensor):\n",
    "                z = z.cpu().numpy()\n",
    "            if not isinstance(z, np.ndarray):\n",
    "                z = np.array(z)\n",
    "            \n",
    "            # Ensure z is 1D array of length 30\n",
    "            if z.ndim > 1:\n",
    "                z = z.flatten()\n",
    "            if len(z) != 30:\n",
    "                print(f\"Warning: Latent vector has shape {z.shape}, expected 30, skipping\")\n",
    "                continue\n",
    "            \n",
    "            latents_list.append(z)\n",
    "            rmsd_list.append(float(rmsd))\n",
    "            energy_list.append(float(energy) if not np.isnan(energy) else 0.0)\n",
    "            gen_list.append(int(gen))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    latents = np.array(latents_list)\n",
    "    rmsd = np.array(rmsd_list)\n",
    "    energy = np.array(energy_list)\n",
    "    gen = np.array(gen_list)\n",
    "    \n",
    "    print(f\"\\nLoaded {len(latents)} samples\")\n",
    "    print(f\"Latent vectors shape: {latents.shape}\")\n",
    "    print(f\"RMSD range: {rmsd.min():.2f} - {rmsd.max():.2f} Å\")\n",
    "    print(f\"Good poses (RMSD < 2Å): {(rmsd < 2.0).sum()} ({(rmsd < 2.0).mean()*100:.1f}%)\")\n",
    "    print(f\"Poor poses (RMSD >= 2Å): {(rmsd >= 2.0).sum()} ({(rmsd >= 2.0).mean()*100:.1f}%)\")\n",
    "    print(f\"Generation range: {gen.min()} - {gen.max()}\")\n",
    "    \n",
    "    return latents, rmsd, energy, gen\n",
    "\n",
    "# Load data from all protein systems\n",
    "data_base_dir = '../data'\n",
    "data_dirs = [\n",
    "    'pim1_3vbt_pim1_4lmu_optimization',\n",
    "    'pim1_4lmu_pim1_4bzo_withRL',\n",
    "    'rho_2esm_rho_2etk_optimization'\n",
    "]\n",
    "\n",
    "# Load data from all directories\n",
    "all_latents = []\n",
    "all_rmsd = []\n",
    "all_energy = []\n",
    "all_gen = []\n",
    "\n",
    "for data_subdir in data_dirs:\n",
    "    data_dir = os.path.join(data_base_dir, data_subdir)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading from: {data_subdir}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    latents, rmsd, energy, gen = load_pickle_data(data_dir, max_gen=7)\n",
    "    all_latents.append(latents)\n",
    "    all_rmsd.append(rmsd)\n",
    "    all_energy.append(energy)\n",
    "    all_gen.append(gen)\n",
    "\n",
    "# Concatenate all data\n",
    "latents = np.concatenate(all_latents, axis=0)\n",
    "rmsd = np.concatenate(all_rmsd, axis=0)\n",
    "energy = np.concatenate(all_energy, axis=0)\n",
    "gen = np.concatenate(all_gen, axis=0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"COMBINED DATA SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total samples: {len(latents)}\")\n",
    "print(f\"Latent vectors shape: {latents.shape}\")\n",
    "print(f\"RMSD range: {rmsd.min():.2f} - {rmsd.max():.2f} Å\")\n",
    "print(f\"Good poses (RMSD < 2Å): {(rmsd < 2.0).sum()} ({(rmsd < 2.0).mean()*100:.1f}%)\")\n",
    "print(f\"Poor poses (RMSD >= 2Å): {(rmsd >= 2.0).sum()} ({(rmsd >= 2.0).mean()*100:.1f}%)\")\n",
    "print(f\"Generation range: {gen.min()} - {gen.max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Normalization and Train/Val Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Z-score normalization and stratified train/val split\n",
    "\n",
    "# Create binary labels for stratification (RMSD < 2Å = good pose)\n",
    "is_good = (rmsd < 2.0).astype(int)\n",
    "\n",
    "# Z-score normalization\n",
    "scaler = StandardScaler()\n",
    "latents_normalized = scaler.fit_transform(latents)\n",
    "print(f\"Normalized latents shape: {latents_normalized.shape}\")\n",
    "print(f\"Normalized latents mean: {latents_normalized.mean(axis=0).mean():.6f}\")\n",
    "print(f\"Normalized latents std: {latents_normalized.std(axis=0).mean():.6f}\")\n",
    "\n",
    "# 70/30 train/val split stratified by RMSD < 2Å label\n",
    "X_train, X_val, y_train_rmsd, y_val_rmsd, y_train_label, y_val_label = train_test_split(\n",
    "    latents_normalized, rmsd, is_good,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=is_good\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train)} samples\")\n",
    "print(f\"  Good poses: {(y_train_label == 1).sum()} ({(y_train_label == 1).mean()*100:.1f}%)\")\n",
    "print(f\"  Poor poses: {(y_train_label == 0).sum()} ({(y_train_label == 0).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nVal set: {len(X_val)} samples\")\n",
    "print(f\"  Good poses: {(y_val_label == 1).sum()} ({(y_val_label == 1).mean()*100:.1f}%)\")\n",
    "print(f\"  Poor poses: {(y_val_label == 0).sum()} ({(y_val_label == 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "print(f\"\\nTensor shapes:\")\n",
    "print(f\"X_train_tensor: {X_train_tensor.shape}\")\n",
    "print(f\"X_val_tensor: {X_val_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TopK Sparse Autoencoder Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TopK Sparse Autoencoder following Adams et al. (2025)\n",
    "# Architecture: 30D input -> 120D hidden (TopK K=6) -> 30D output\n",
    "\n",
    "class TopKSAE(nn.Module):\n",
    "    \"\"\"\n",
    "    TopK Sparse Autoencoder.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (30D) -> Linear -> Hidden (120D) -> TopK(K=6) -> Linear -> Output (30D)\n",
    "    \n",
    "    The TopK operation keeps only the K largest activations per sample, zeroing out the rest.\n",
    "    This enforces sparsity without requiring a sparsity penalty term.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=30, hidden_dim=120, k=6):\n",
    "        super(TopKSAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.k = k  # Number of active features (K=6 gives ~5% sparsity for 120D)\n",
    "        \n",
    "        # Encoder: input -> hidden\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Decoder: hidden -> output\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier uniform initialization.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.encoder.weight)\n",
    "        nn.init.zeros_(self.encoder.bias)\n",
    "        nn.init.xavier_uniform_(self.decoder.weight)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with TopK sparsity.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, input_dim]\n",
    "        \n",
    "        Returns:\n",
    "            reconstructed: Reconstructed output [batch_size, input_dim]\n",
    "            h_sparse: Sparse hidden activations [batch_size, hidden_dim]\n",
    "        \"\"\"\n",
    "        # Encode to hidden layer\n",
    "        h = self.encoder(x)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Apply TopK: keep only K largest activations per sample\n",
    "        # topk_vals: [batch_size, k] - the K largest values\n",
    "        # topk_indices: [batch_size, k] - indices of the K largest values\n",
    "        topk_vals, topk_indices = torch.topk(h, self.k, dim=-1)\n",
    "        \n",
    "        # Create sparse hidden representation: zeros everywhere except top K\n",
    "        h_sparse = torch.zeros_like(h)\n",
    "        h_sparse.scatter_(-1, topk_indices, topk_vals)\n",
    "        \n",
    "        # Decode to output\n",
    "        reconstructed = self.decoder(h_sparse)  # [batch_size, input_dim]\n",
    "        \n",
    "        return reconstructed, h_sparse\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to sparse hidden representation.\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        topk_vals, topk_indices = torch.topk(h, self.k, dim=-1)\n",
    "        h_sparse = torch.zeros_like(h)\n",
    "        h_sparse.scatter_(-1, topk_indices, topk_vals)\n",
    "        return h_sparse\n",
    "\n",
    "# Initialize model\n",
    "model = TopKSAE(input_dim=30, hidden_dim=120, k=6).to(device)\n",
    "print(f\"Model initialized:\")\n",
    "print(f\"  Input dim: {model.input_dim}\")\n",
    "print(f\"  Hidden dim: {model.hidden_dim}\")\n",
    "print(f\"  K (sparsity): {model.k} ({model.k/model.hidden_dim*100:.1f}% active)\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_input = X_train_tensor[:5]\n",
    "    test_recon, test_hidden = model(test_input)\n",
    "    print(f\"\\nTest forward pass:\")\n",
    "    print(f\"  Input shape: {test_input.shape}\")\n",
    "    print(f\"  Hidden shape: {test_hidden.shape}\")\n",
    "    print(f\"  Reconstructed shape: {test_recon.shape}\")\n",
    "    print(f\"  Hidden sparsity: {(test_hidden == 0).float().mean().item()*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop: 100 epochs, Adam optimizer, lr=1e-3\n",
    "\n",
    "# Setup\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "n_epochs = 100\n",
    "\n",
    "print(f\"Starting training for {n_epochs} epochs...\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    # Shuffle training data\n",
    "    indices = torch.randperm(len(X_train_tensor))\n",
    "    \n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        batch_x = X_train_tensor[batch_indices]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, h_sparse = model(batch_x)\n",
    "        \n",
    "        # Loss: MSE between input and reconstruction\n",
    "        loss = criterion(reconstructed, batch_x)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    avg_train_loss = train_loss / n_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    n_val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_val_tensor), batch_size):\n",
    "            batch_x = X_val_tensor[i:i+batch_size]\n",
    "            reconstructed, h_sparse = model(batch_x)\n",
    "            loss = criterion(reconstructed, batch_x)\n",
    "            val_loss += loss.item()\n",
    "            n_val_batches += 1\n",
    "    \n",
    "    avg_val_loss = val_loss / n_val_batches\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: Train Loss = {avg_train_loss:.6f}, Val Loss = {avg_val_loss:.6f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'topk_sae.pt')\n",
    "print(\"Model saved to 'topk_sae.pt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loss Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.title('TopK SAE Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Get Sparse Hidden Activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Extract sparse hidden activations for all data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m all_features \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Process in batches to avoid memory issues\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract sparse hidden activations for all data\n",
    "model.eval()\n",
    "all_features = []\n",
    "\n",
    "# Process in batches to avoid memory issues\n",
    "batch_size_extract = 256\n",
    "all_latents = torch.FloatTensor(latents_normalized).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(all_latents), batch_size_extract):\n",
    "        batch_x = all_latents[i:i+batch_size_extract]\n",
    "        features = model.encode(batch_x)  # [batch_size, 120]\n",
    "        all_features.append(features.cpu().numpy())\n",
    "\n",
    "# Concatenate all features\n",
    "all_features = np.concatenate(all_features, axis=0)\n",
    "print(f\"Extracted features shape: {all_features.shape}\")\n",
    "print(f\"Feature sparsity: {(all_features == 0).sum() / all_features.size * 100:.1f}%\")\n",
    "print(f\"Average active features per sample: {(all_features != 0).sum(axis=1).mean():.2f} (expected: {model.k})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis: Correlations with RMSD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m feature_correlations \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m feature_pvalues \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m feat_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(all_features\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     corr, pval \u001b[39m=\u001b[39m spearmanr(all_features[:, feat_idx], rmsd)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bridget/Desktop/projects/laloo-sae/notebooks/v1.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     feature_correlations\u001b[39m.\u001b[39mappend(corr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate Spearman correlations between each feature and RMSD\n",
    "feature_correlations = []\n",
    "feature_pvalues = []\n",
    "\n",
    "for feat_idx in range(all_features.shape[1]):\n",
    "    corr, pval = spearmanr(all_features[:, feat_idx], rmsd)\n",
    "    feature_correlations.append(corr)\n",
    "    feature_pvalues.append(pval)\n",
    "\n",
    "feature_correlations = np.array(feature_correlations)\n",
    "feature_pvalues = np.array(feature_pvalues)\n",
    "\n",
    "# Sort features by absolute correlation\n",
    "sorted_indices = np.argsort(np.abs(feature_correlations))[::-1]\n",
    "\n",
    "print(\"Top 10 features by absolute correlation with RMSD:\")\n",
    "print(\"Feature | Correlation | P-value\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(min(10, len(sorted_indices))):\n",
    "    idx = sorted_indices[i]\n",
    "    print(f\"  {idx:3d}   |  {feature_correlations[idx]:+7.4f}   | {feature_pvalues[idx]:.2e}\")\n",
    "\n",
    "# Features with significant correlations (p < 0.05)\n",
    "significant_features = np.where(feature_pvalues < 0.05)[0]\n",
    "print(f\"\\nSignificant features (p < 0.05): {len(significant_features)} out of {all_features.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations: Feature Activation Heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature activation heatmap: top 10 features, sorted by RMSD\n",
    "top_n_features = 10\n",
    "top_feature_indices = sorted_indices[:top_n_features]\n",
    "\n",
    "# Sort samples by RMSD\n",
    "sorted_by_rmsd = np.argsort(rmsd)\n",
    "n_samples_plot = min(500, len(rmsd))  # Plot up to 500 samples for clarity\n",
    "sample_indices = sorted_by_rmsd[::max(1, len(rmsd)//n_samples_plot)][:n_samples_plot]\n",
    "\n",
    "# Extract feature activations for top features and selected samples\n",
    "heatmap_data = all_features[np.ix_(sample_indices, top_feature_indices)]\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data.T, \n",
    "            xticklabels=[f\"RMSD={rmsd[i]:.2f}\" for i in sample_indices[::max(1, len(sample_indices)//20)]],\n",
    "            yticklabels=[f\"Feat {idx}\" for idx in top_feature_indices],\n",
    "            cmap='RdYlBu_r', center=0,\n",
    "            cbar_kws={'label': 'Activation'})\n",
    "plt.xlabel('Samples (sorted by RMSD)', fontsize=11)\n",
    "plt.ylabel('Top Features (by |correlation|)', fontsize=11)\n",
    "plt.title('Top 10 SAE Feature Activations vs RMSD', fontsize=13, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations: Scatter Plots of Top 3 Features vs RMSD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots of top 3 features vs RMSD\n",
    "top_3_features = sorted_indices[:3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, feat_idx in enumerate(top_3_features):\n",
    "    ax = axes[i]\n",
    "    scatter = ax.scatter(all_features[:, feat_idx], rmsd, \n",
    "                        c=rmsd, cmap='viridis', alpha=0.5, s=10)\n",
    "    ax.set_xlabel(f'Feature {feat_idx} Activation', fontsize=11)\n",
    "    ax.set_ylabel('RMSD (Å)', fontsize=11)\n",
    "    ax.set_title(f'Feature {feat_idx}\\n(corr={feature_correlations[feat_idx]:.3f})', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label='RMSD (Å)')\n",
    "\n",
    "plt.suptitle('Top 3 SAE Features vs RMSD', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classifier: Logistic Regression on SAE Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression on SAE features\n",
    "# Binary classification: RMSD < 2Å (good) vs RMSD >= 2Å (poor)\n",
    "\n",
    "# Prepare data\n",
    "is_good_pose = (rmsd < 2.0).astype(int)\n",
    "\n",
    "# Split features into train/val (same split as SAE training)\n",
    "X_train_features = all_features[:len(X_train)]\n",
    "X_val_features = all_features[len(X_train):len(X_train)+len(X_val)]\n",
    "y_train_binary = is_good_pose[:len(X_train)]\n",
    "y_val_binary = is_good_pose[len(X_train):len(X_train)+len(X_val)]\n",
    "\n",
    "print(f\"Training logistic regression on SAE features...\")\n",
    "print(f\"Train: {len(X_train_features)} samples, {y_train_binary.sum()} good poses\")\n",
    "print(f\"Val: {len(X_val_features)} samples, {y_val_binary.sum()} good poses\")\n",
    "\n",
    "# Train logistic regression\n",
    "lr_sae = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_sae.fit(X_train_features, y_train_binary)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_sae = lr_sae.predict_proba(X_train_features)[:, 1]\n",
    "y_val_pred_sae = lr_sae.predict_proba(X_val_features)[:, 1]\n",
    "\n",
    "# Calculate auPR\n",
    "train_aupr_sae = average_precision_score(y_train_binary, y_train_pred_sae)\n",
    "val_aupr_sae = average_precision_score(y_val_binary, y_val_pred_sae)\n",
    "\n",
    "print(f\"\\nSAE Features Classifier:\")\n",
    "print(f\"  Train auPR: {train_aupr_sae:.4f}\")\n",
    "print(f\"  Val auPR: {val_aupr_sae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Comparison: Logistic Regression on Raw Latent Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression on raw 30D latent vectors (no SAE) for comparison\n",
    "X_train_raw = latents_normalized[:len(X_train)]\n",
    "X_val_raw = latents_normalized[len(X_train):len(X_train)+len(X_val)]\n",
    "\n",
    "print(f\"Training logistic regression on raw latent vectors...\")\n",
    "print(f\"Train: {len(X_train_raw)} samples\")\n",
    "print(f\"Val: {len(X_val_raw)} samples\")\n",
    "\n",
    "# Train logistic regression\n",
    "lr_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_raw.fit(X_train_raw, y_train_binary)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_raw = lr_raw.predict_proba(X_train_raw)[:, 1]\n",
    "y_val_pred_raw = lr_raw.predict_proba(X_val_raw)[:, 1]\n",
    "\n",
    "# Calculate auPR\n",
    "train_aupr_raw = average_precision_score(y_train_binary, y_train_pred_raw)\n",
    "val_aupr_raw = average_precision_score(y_val_binary, y_val_pred_raw)\n",
    "\n",
    "print(f\"\\nRaw Latent Vectors Classifier:\")\n",
    "print(f\"  Train auPR: {train_aupr_raw:.4f}\")\n",
    "print(f\"  Val auPR: {val_aupr_raw:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive results summary\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. MODEL ARCHITECTURE:\")\n",
    "print(f\"   - Input dimension: 30D\")\n",
    "print(f\"   - Hidden dimension: 120D\")\n",
    "print(f\"   - TopK sparsity: K={model.k} ({model.k/model.hidden_dim*100:.1f}% active)\")\n",
    "print(f\"   - Final train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"   - Final val loss: {val_losses[-1]:.6f}\")\n",
    "\n",
    "print(f\"\\n2. TOP CORRELATED FEATURES WITH RMSD:\")\n",
    "print(f\"   (Features with highest |Spearman correlation|)\")\n",
    "for i in range(min(10, len(sorted_indices))):\n",
    "    idx = sorted_indices[i]\n",
    "    sig = \"*\" if feature_pvalues[idx] < 0.05 else \" \"\n",
    "    print(f\"   {sig} Feature {idx:3d}: corr = {feature_correlations[idx]:+7.4f}, p = {feature_pvalues[idx]:.2e}\")\n",
    "\n",
    "print(f\"\\n3. CLASSIFICATION PERFORMANCE (auPR):\")\n",
    "print(f\"   Raw 30D Latent Vectors:\")\n",
    "print(f\"     Train auPR: {train_aupr_raw:.4f}\")\n",
    "print(f\"     Val auPR:   {val_aupr_raw:.4f}\")\n",
    "print(f\"   SAE Features (120D, TopK={model.k}):\")\n",
    "print(f\"     Train auPR: {train_aupr_sae:.4f}\")\n",
    "print(f\"     Val auPR:   {val_aupr_sae:.4f}\")\n",
    "print(f\"   Improvement: {val_aupr_sae - val_aupr_raw:+.4f} ({((val_aupr_sae/val_aupr_raw - 1)*100):+.1f}%)\")\n",
    "\n",
    "print(f\"\\n4. DATA STATISTICS:\")\n",
    "print(f\"   Total samples: {len(latents)}\")\n",
    "print(f\"   Good poses (RMSD < 2Å): {(rmsd < 2.0).sum()} ({(rmsd < 2.0).mean()*100:.1f}%)\")\n",
    "print(f\"   Poor poses (RMSD >= 2Å): {(rmsd >= 2.0).sum()} ({(rmsd >= 2.0).mean()*100:.1f}%)\")\n",
    "print(f\"   RMSD range: {rmsd.min():.2f} - {rmsd.max():.2f} Å\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laloo_sae_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
